{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network class with better functions\n",
    "\n",
    "In this notebok, I am going to clean up some of code from the prior notebooks and wrap the whole thing up in a nice class structure. \n",
    "\n",
    "Additionally, I have added the ability to scale up to more neurons. Lastly, I demonstrate the network running some actually useful classification tasks using scikit's Iris and Breast cancer datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class neuralnetwork(object):\n",
    "    def __init__(self, sizes):\n",
    "        np.random.seed(42)\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.zs = []\n",
    "        self.activations = []\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        #print(\"Initial Biases: \\n{}\".format(self.biases))\n",
    "        #print(\"Initial Weights: \\n{}\".format(self.weights))\n",
    "    def reinitialize(self):\n",
    "        self.zs = []\n",
    "        self.activations = []\n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]    \n",
    "        #print(\"Initial Biases: \\n{}\".format(self.biases))\n",
    "        #print(\"Initial Weights: \\n{}\".format(self.weights))\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    def cost_function(self, y, y_predicted):\n",
    "        return .5*(y - y_predicted)**2\n",
    "    def cost_function_deriv(self, y_predicted, y):\n",
    "        #print(\"calculating cost_function_deriv: \\ny_predicted is:\\n{}\\n and y is:\\n{}\\n\".format(y_predicted, y))\n",
    "        return y_predicted - y\n",
    "    def activation_function(self, z):\n",
    "        return self.sigmoid(z)\n",
    "    def activation_function_deriv(self, z):\n",
    "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        #print(\"-------in the feedforward-------------\")\n",
    "        self.zs = []\n",
    "        self.activations = [a]\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, a)+b\n",
    "            self.zs.append(z)\n",
    "            a = self.activation_function(z)\n",
    "            self.activations.append(a)\n",
    "        #print(\"Observed Vectors:\\n{}\".format(self.zs))\n",
    "        #print(\"Observed Activations:\\n{}\".format(self.activations))\n",
    "        #print(\"feedforward returns: {}\".format(a))\n",
    "        #print(\"-----End feedforward-------\\n\")\n",
    "        return a\n",
    "    \n",
    "    def backprop(self, y):\n",
    "        #print(\"-------in the backprop-------------\")\n",
    "        #print(\"zs:\\n{}\\nactivations:\\n{}\\n\".format(self.zs, self.activations))\n",
    "        biasgrads = [np.zeros(b.shape) for b in self.biases]\n",
    "        weightgrads = [np.zeros(w.shape) for w in self.weights]\n",
    "        #print(\"The zeros:\\nbiasgrads:\\n{}\\nweightgrads:\\n{}\".format(biasgrads, weightgrads))\n",
    "        #print(\"Making the delta. \\ncost_function_deriv is:\\n{}\\nactivation_function_deriv is:\\n{}\\n\\n\".format(self.cost_function_deriv(self.activations[-1], y), self.activation_function_deriv(self.zs[-1])))\n",
    "        delta = self.cost_function_deriv(self.activations[-1], y) * self.activation_function_deriv(self.zs[-1])\n",
    "        #print(\"delta:{}\".format(delta))\n",
    "        biasgrads[-1] = np.sum(delta)\n",
    "        weightgrads[-1] = np.dot(delta, self.activations[-2].transpose())\n",
    "        #print(\"after delta:\\nbiasgrads:\\n{}\\nweightgrads:\\n{}\".format(biasgrads, weightgrads))\n",
    "        #print(\"Now the for loop...\")\n",
    "        for l in range(2,self.num_layers):\n",
    "            z = self.zs[-l]\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * self.activation_function_deriv(z)\n",
    "            biasgrads[-l] = np.sum(delta)\n",
    "            weightgrads[-l] = np.dot(delta, self.activations[-l-1].transpose())\n",
    "        #print(\"Backprop returning:\\nbias grad:{}\\nweightgrads:\\n{}\".format(biasgrads, weightgrads))    \n",
    "        #print(\"--------End Backprop--------------------\\n\")\n",
    "        return biasgrads, weightgrads\n",
    "    \n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        self.reinitialize()\n",
    "        #print(\"Bias defaults: {}\".format(self.biases))\n",
    "        #print(\"Weight defaults: {}\\n\\n\".format(self.weights))  \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            #print(\"------------------------Running epoch {}--------------------------\\n\".format(epoch))\n",
    "            biasgradients = [np.zeros(b.shape) for b in self.biases]\n",
    "            weightgradients = [np.zeros(w.shape) for w in self.weights]   \n",
    "            \n",
    "            #print(\"Epoch Bias gradient default: {}\".format(biasgradients))\n",
    "            #print(\"Epoch Weight gradient default: {}\\n\".format(biasgradients))    \n",
    "            \n",
    "            #forward propagation\n",
    "            output = self.feedforward(X.T)\n",
    "            #print(\"Output: {}\".format(output))\n",
    "            \n",
    "            #compute the cost\n",
    "            cost = self.cost_function(y.T, output)\n",
    "            #print(\"Calculated Cost: {}\\n\".format(cost))\n",
    "            \n",
    "            #get the gradients\n",
    "            #print(\"Zs: {}\".format(self.zs))\n",
    "            #print(\"Activations: {}\".format(self.activations))\n",
    "                    \n",
    "            delta_biasgradients, delta_weightgradients = self.backprop(y.T)\n",
    "            \n",
    "            #print(\"delta Bias gradients: {}\".format(delta_biasgradients))\n",
    "            #print(\"delta Weight gradients: {}\\n\\n\".format(delta_weightgradients))\n",
    "            \n",
    "            biasgradients = [bgrad + dbgrad for bgrad, dbgrad in zip(biasgradients, delta_biasgradients)]\n",
    "            weightgradients = [wgrad + dwgrad for wgrad, dwgrad in zip(weightgradients, delta_weightgradients)]\n",
    "            \n",
    "            #print(\"Bias gradients: {}\".format(biasgradients))\n",
    "            #print(\"Weight gradients: {}\\n\\n\".format(weightgradients))\n",
    "                      \n",
    "            #update the weights and biases\n",
    "            self.biases = [b - learning_rate * biasgrad for b, biasgrad in zip(self.biases, biasgradients)]\n",
    "            self.weights = [w - learning_rate * weightgrad for w, weightgrad in zip(self.weights, weightgradients)]\n",
    "\n",
    "            #print(\"Biases: {}\".format(self.biases))\n",
    "            #print(\"Weights: {}\\n\\n\".format(self.weights))\n",
    "            \n",
    "            #print(\"----------------------------End Epoch {}----------------------------\\n\\n\".format(epoch))\n",
    "        #print(\"Final Cost: {}\".format(cost))\n",
    "        #print(\"Final Biases: {}\".format(self.biases))\n",
    "        #print(\"Final Weights: {}\\n\\n\".format(self.weights))\n",
    "    def predict(self, X):\n",
    "        return self.feedforward(X.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = neuralnetwork([1,1,1])\n",
    "\n",
    "X = np.array([\n",
    "    [0],\n",
    "    [1]\n",
    "])\n",
    "\n",
    "\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [1]\n",
    "])\n",
    "#set up the learning rate\n",
    "lr = 0.2\n",
    "#set up the number of epochs\n",
    "e = 1\n",
    "\n",
    "net.train(X, y, lr, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = neuralnetwork([2,2,1])\n",
    "\n",
    "X = np.array([\n",
    "    [0,0],\n",
    "    [0,1],\n",
    "    [1,0],\n",
    "    [1,1]\n",
    "])\n",
    "\n",
    "\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1],\n",
    "])\n",
    "#set up the learning rate\n",
    "lr = 0.2\n",
    "#set up the number of epochs\n",
    "e = 1\n",
    "\n",
    "net.train(X, y, lr, e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Predictions\n",
      "Should be 1: [[ 0.93325716]]\n",
      "Should be 0: [[ 0.07671716]]\n"
     ]
    }
   ],
   "source": [
    "net = neuralnetwork([3,2,1])\n",
    "\n",
    "\n",
    "#Demonstrate significant weight on the first input variable, \n",
    "#and none on the other two. \n",
    "X = np.array([\n",
    "    [0,0,0],\n",
    "    [0,0,1],\n",
    "    [0,1,0],\n",
    "    [0,1,1],\n",
    "    [1,0,0],\n",
    "    [1,0,1],\n",
    "    [1,1,0],\n",
    "    [1,1,1],\n",
    "\n",
    "])\n",
    "\n",
    "#for this goal these all match the first input\n",
    "y = np.array([\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [1],\n",
    "    [1],\n",
    "])\n",
    "#set up the learning rate\n",
    "lr = 0.20\n",
    "#set up the number of epochs\n",
    "e = 500\n",
    "\n",
    "net.train(X, y, lr, e)\n",
    "\n",
    "print(\"--------------------Predictions\")\n",
    "\n",
    "print(\"Should be 1: {}\".format(net.predict(np.array([[1,0,0]]))))\n",
    "\n",
    "print(\"Should be 0: {}\".format(net.predict(np.array([[0,1,1]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "[[  4.65426872e-05   9.99999567e-01   9.99999922e-01   9.99999897e-01\n",
      "    9.99999924e-01   9.99999882e-01   9.99999588e-01   9.99999946e-01\n",
      "    9.99999905e-01   9.99999947e-01   9.88579609e-01   9.99999654e-01\n",
      "    9.99999954e-01   4.02615709e-01   7.11449704e-01   5.44290209e-05\n",
      "    9.99965246e-01   4.48697725e-05   4.54146584e-05   4.56131847e-05\n",
      "    6.00961003e-05   5.06977137e-05   9.99999635e-01   9.99999933e-01\n",
      "    4.56235956e-05   9.99999931e-01   9.99999954e-01   8.46061082e-05\n",
      "    9.99999922e-01   4.49945333e-05   9.99999959e-01   4.61650633e-05\n",
      "    9.99568896e-01   4.79118878e-05   9.99999948e-01   5.12566012e-05\n",
      "    9.99999268e-01   4.97206406e-05   9.99999586e-01   4.32018906e-05\n",
      "    1.78340102e-03   9.99999955e-01   2.93014069e-04   9.99999949e-01\n",
      "    9.99995450e-01   4.50643844e-05   9.99999949e-01   9.99999895e-01\n",
      "    9.99999952e-01   4.71270393e-05   5.90678740e-05   1.44708615e-04\n",
      "    4.40199839e-05   9.99999792e-01   9.99999957e-01   9.99999761e-01\n",
      "    9.99999940e-01   9.99999749e-01   9.99999902e-01   4.55721437e-05\n",
      "    5.22245556e-05   5.36477040e-05   9.99999963e-01   9.99999908e-01\n",
      "    5.15305960e-05   9.99998968e-01   4.45332027e-05   4.59955413e-05\n",
      "    4.64815609e-05   9.99999933e-01   9.94304073e-01   4.31039532e-05\n",
      "    9.99999939e-01   1.04496763e-02   4.56208885e-05   9.99999684e-01\n",
      "    9.99999672e-01   9.99986402e-01   9.99999949e-01   9.99999962e-01\n",
      "    9.38948512e-05   4.31549901e-05   4.55225217e-05   9.99999955e-01\n",
      "    6.20130908e-05   9.99999934e-01   9.99999957e-01   9.99999948e-01\n",
      "    4.58360323e-05   4.45241870e-05   9.99999955e-01   9.90003116e-01\n",
      "    3.41417300e-01   4.49688855e-05   9.99999937e-01   9.99999904e-01\n",
      "    4.60289557e-05   9.99998776e-01   9.99999951e-01   9.99999955e-01\n",
      "    9.99999940e-01   9.99999905e-01   9.99999785e-01   9.99999645e-01\n",
      "    4.38778134e-05   9.99979789e-01   4.56189379e-05   9.99999417e-01\n",
      "    9.17100914e-01   7.49093938e-01   9.99999814e-01   4.47352770e-05\n",
      "    4.53654299e-05   9.99999689e-01   9.99999956e-01   9.99999943e-01\n",
      "    9.98765713e-01   9.99999960e-01   9.99999890e-01   9.99999787e-01\n",
      "    9.99999933e-01   9.99999909e-01   1.79892627e-04   9.99999182e-01\n",
      "    4.31481734e-05   9.98656677e-01   9.99260809e-01   9.99999928e-01\n",
      "    9.99999930e-01   9.99999949e-01   4.60273006e-05   9.99999920e-01\n",
      "    9.99999948e-01   9.99948788e-01   9.99913914e-01   9.99999950e-01\n",
      "    9.89595854e-01   8.70578668e-01   4.44188683e-05   9.99999906e-01\n",
      "    9.99999785e-01   9.99999814e-01   4.52624791e-05]]\n",
      "Actual:\n",
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 0 1 0 1 1 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 0\n",
      " 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 0 1 0 1 0 0 1\n",
      " 0 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 1 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy  as np\n",
    "from  sklearn import datasets\n",
    "\n",
    "# Importing the dataset\n",
    "bcancer = datasets.load_breast_cancer()\n",
    "X = bcancer.data\n",
    "y = bcancer.target\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "net = neuralnetwork([30,20,10,1])\n",
    "#set up the learning rate\n",
    "lr = 0.20\n",
    "#set up the number of epochs\n",
    "e = 500\n",
    "net.train(X_train, y_train, lr, e)\n",
    "\n",
    "print(\"Predicted:\")\n",
    "print(net.predict(X_test))\n",
    "print(\"Actual:\")\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:\n",
      "[[  1.29207056e-03   2.77715322e-03   9.94256739e-01]\n",
      " [  1.87267834e-02   9.88722909e-01   7.46471594e-03]\n",
      " [  9.88308674e-01   5.56403663e-03   1.14001310e-03]\n",
      " [  9.58396305e-04   4.80031331e-03   9.93180454e-01]\n",
      " [  9.87810959e-01   6.29659740e-03   1.12952738e-03]\n",
      " [  1.34985850e-03   2.50497315e-03   9.94573941e-01]\n",
      " [  9.87994841e-01   5.99134853e-03   1.13697935e-03]\n",
      " [  1.87483209e-02   9.85008084e-01   9.30218495e-03]\n",
      " [  1.77269791e-02   9.83652353e-01   1.04996555e-02]\n",
      " [  2.00493753e-02   9.87506197e-01   7.56829367e-03]\n",
      " [  5.53151338e-04   2.73541559e-02   9.84165489e-01]\n",
      " [  1.93836753e-02   9.86111541e-01   8.49052849e-03]\n",
      " [  1.68971409e-02   9.82805721e-01   1.14664426e-02]\n",
      " [  1.76347965e-02   9.83073577e-01   1.08277126e-02]\n",
      " [  1.49778999e-02   9.77567535e-01   1.57741416e-02]\n",
      " [  9.87271950e-01   7.95836149e-03   1.08021975e-03]\n",
      " [  1.64976126e-02   9.80972170e-01   1.26584098e-02]\n",
      " [  1.60155110e-02   9.81543423e-01   1.27520841e-02]\n",
      " [  9.83745998e-01   1.45579925e-02   1.02739016e-03]\n",
      " [  9.86967476e-01   7.47695608e-03   1.06956709e-03]\n",
      " [  9.49158423e-04   9.88314200e-03   9.87928439e-01]\n",
      " [  1.35895656e-02   9.72460062e-01   2.02159580e-02]\n",
      " [  9.87285344e-01   6.81287866e-03   1.11907562e-03]\n",
      " [  9.87230258e-01   8.41908446e-03   1.06922395e-03]\n",
      " [  4.29811392e-03   5.92838389e-01   3.67286178e-01]\n",
      " [  9.89023101e-01   4.34769188e-03   1.22666972e-03]\n",
      " [  9.84246089e-01   1.03612274e-02   1.03556599e-03]\n",
      " [  1.97762903e-02   9.87048230e-01   7.88982530e-03]\n",
      " [  2.03187990e-02   9.87938761e-01   7.25967892e-03]\n",
      " [  9.83812860e-01   1.19360044e-02   1.03513563e-03]\n",
      " [  7.68260150e-04   2.07203445e-02   9.82709816e-01]\n",
      " [  1.10684181e-02   9.59101847e-01   3.29661239e-02]\n",
      " [  9.87565502e-01   6.63960046e-03   1.11405746e-03]\n",
      " [  2.94765255e-03   4.32168873e-01   5.64252572e-01]\n",
      " [  1.23626044e-03   2.92328712e-03   9.94251718e-01]\n",
      " [  1.89398170e-02   9.86019488e-01   8.73074293e-03]\n",
      " [  9.81948287e-01   1.32310000e-02   1.01291087e-03]\n",
      " [  1.05658543e-03   9.05366965e-02   9.31852862e-01]]\n",
      "Actual:\n",
      "[[ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  1.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy  as np\n",
    "from  sklearn import datasets\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = datasets.load_iris()\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "#encode the result set\n",
    "enc = OneHotEncoder()\n",
    "y_encoded = enc.fit_transform(np.expand_dims(y, axis=1))\n",
    "y = y_encoded.toarray()\n",
    "\n",
    "#split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "#scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "#neural network\n",
    "\n",
    "net = neuralnetwork([4,8,8,3])\n",
    "#set up the learning rate\n",
    "lr = 0.20\n",
    "#set up the number of epochs\n",
    "e = 500\n",
    "net.train(X_train, y_train, lr, e)\n",
    "\n",
    "print(\"Predicted:\")\n",
    "print(net.predict(X_test).T)\n",
    "print(\"Actual:\")\n",
    "print(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datacamp]",
   "language": "python",
   "name": "conda-env-datacamp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
